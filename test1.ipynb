{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcE2EfvchieDKzy+Q9F0Qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y79tian/Machine-learning-Coursera/blob/master/test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQfqivGz0W3t"
      },
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, roc_curve, auc, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "\n",
        "class XGBoostClassifier:\n",
        "  '''Wrapper class for XGBoost Classifer'''\n",
        "\n",
        "  def __init__(self, fileName='data.pq', aggInterval=['1H'], predictionInterval='1_H', \n",
        "               testRatio=0.2, maxDepth=9, learningRate=0.15, subsample=0.9, colsample_bytree=0.3, objective='binary:logistic', numIters=100, earlyStoppingRounds=8):\n",
        "    '''Initalize XGBoost Model given parameters\n",
        "      Parameters:\n",
        "        - fileName (str): name of file to read in data from  (optional: default = 'data.pq')\n",
        "        - aggInterval ([str]): list of aggregation time intervals (ex: ['1H'] = 1 hour, ['1D'] = 1 day, ['5H'] = 5 hours, ['1T'] = 1 minute, ['1T', '1H] = 1 minute and 1 hour)  (optional: default = ['1H'], 1 hour) (can pass [] for no aggregation)\n",
        "            for more info about time interval options: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects\n",
        "            NOTE: limit 2 time intervals for now\n",
        "        - predictionInterval (str): prediction time offset (ex: predict craving in '3_H' = 3 hours, '1_H' = 1 day)  (optional: default = '1_H', 1 hour) (can pass '' for no offset, predict craving at current time)\n",
        "            NOTE: prediction interval must be given in either hours (H), minutes (M), or days (D)\n",
        "        - testRatio (float): ratio for test data in train-test split (ex: 0.2 = 20%, 80% train)  (optional: default = 0.2)\n",
        "        - maxDepth (int): maximum depth of tree  (optional: default = 9)\n",
        "        - learningRate (float): learning rate of XGB model, range [0, 1]  (optional: default = 0.15)\n",
        "        - subsample (float): fraction of observations to be randomly sampled for each tree (optional: default = 0.9)\n",
        "        - colsample_bytree (float): fraction of columns to be randomly sampled for each tree (optional: default = 0.3)\n",
        "        - objective (str): loss function  (optional: default='binary:logistic')\n",
        "            for more info about objective options: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
        "        - numIters (int): number of iterations for XGB model  (optional: default = 100)\n",
        "        - earlyStoppingRounds (int): # of early stopping rounds (optional: default = 8)\n",
        "    '''\n",
        "    self.fileName = fileName\n",
        "    self.aggInterval = aggInterval\n",
        "    self.predictionInterval = predictionInterval\n",
        "    self.testRatio = testRatio\n",
        "    self.maxDepth = maxDepth\n",
        "    self.learningRate = learningRate\n",
        "    self.subsample = subsample\n",
        "    self.colSampleByTree = colsample_bytree\n",
        "    self.objective = objective\n",
        "    self.numIters = numIters  \n",
        "    self.earlyStoppingRounds = earlyStoppingRounds\n",
        "\n",
        "    if len(self.aggInterval) > 2:\n",
        "      raise Exception(\"Number of aggregation intervals is limited to 2\")\n",
        "\n",
        "  def run(self):\n",
        "    '''Run XGBoost model, including:\n",
        "      - reading data\n",
        "      - sampling data\n",
        "      - cleaning data\n",
        "      - aggregating by time interval\n",
        "      - splitting data into training/testing sets\n",
        "      - training\n",
        "      - predicting\n",
        "      - creating log\n",
        "    '''\n",
        "    startTime = time.perf_counter()   # keep track of model run time\n",
        "\n",
        "    # Get data\n",
        "    print(\"=====Reading Raw Data=====\")\n",
        "    dataRaw = readDataToDataFrame(self.fileName)  # get data from file into pd.DataFrame\n",
        "    \n",
        "    # Sample data (manual resampling for now)\n",
        "    # For no sampling, use sampleData = dataRaw\n",
        "    # For sampling, use sampleData= dataRaw.iloc[]  <- sample rows/cols\n",
        "    sampleData = dataRaw  # example for sampling: dataRaw.iloc[827200:900000,:]  rows 827000-90000, all columns\n",
        "    \n",
        "    # Clean data\n",
        "    # if not using pre-cleaned data, need to clean; otherwise, dataCleaned = sampleData\n",
        "    # print(\"=====Cleaning Data=====\")\n",
        "    # dataCleaned = cleanData(sampleData)  # clean data to get it into appropriate format for aggregation/training\n",
        "    dataCleaned = sampleData\n",
        " \n",
        "    # Perform aggregation\n",
        "    # if using pre-aggregated data, no need to aggregated (dataAgg = dataCleaned)\n",
        "    # if need to aggregate, uncomment below\n",
        "    dataAgg = dataCleaned\n",
        "    # if self.aggInterval:\n",
        "    #   print(\"=====Aggregating Data=====\")\n",
        "    #   dataAgg = aggByTimeInterval(dataCleaned, self.aggInterval)  # aggregate data based on time interval\n",
        "\n",
        "    # Add prediction interval labels\n",
        "    # if using pre-labelled data, no need to add prediction labels (predData = dataAgg)\n",
        "    # if need to add prediction labels, uncomment appropriate line below either \n",
        "    # for craving prediction or onset of craving prediction\n",
        "    predData = dataAgg\n",
        "    # if self.predictionInterval:\n",
        "    #   print(\"=====Adding Interval Predictions=====\")\n",
        "        # NOTE: prediction interval '#_type' type=[H, D, M]\n",
        "    #   predData = addPredictionIntervalLabels(dataAgg, self.predictionInterval)  \n",
        "    #   predData = addOnsetPredictionIntervalLabels(dataAgg, self.predictionInterval) \n",
        "\n",
        "    # Format final model data\n",
        "    modelData = predData.iloc[:,:-1]    # remove labels column from training features\n",
        "    modelLabels = predData.iloc[:,-1]   # labels column\n",
        "    print(\"=====Model Ready=====\")\n",
        "\n",
        "    # Split data into training/testing sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(modelData, modelLabels, \n",
        "                                                        test_size=self.testRatio)  # training size: (1 - testRatio), test size: testRatio\n",
        "    print(\"=====Finished train-test split=====\")\n",
        "  \n",
        "    # Normalize features\n",
        "    X_train, X_test = normalize(X_train, X_test)\n",
        "    normalized = True\n",
        "\n",
        "    # Create and fit model\n",
        "    print(\"=====Training=====\")\n",
        "    model = xgb.XGBClassifier(max_depth=self.maxDepth, subsample=self.subsample, \n",
        "                              colsample_bytree=self.colSampleByTree, objective=self.objective, n_estimators=self.numIters, learning_rate=self.learningRate)\n",
        "    evalSet = [(X_train, Y_train), (X_test, Y_test)]\n",
        "    model.fit(X_train, Y_train.values.ravel(), early_stopping_rounds=self.earlyStoppingRounds, eval_metric=[\"error\", \"logloss\"], eval_set=evalSet, verbose=True)\n",
        "    \n",
        "    # Predict with test data\n",
        "    print(\"=====Predicting=====\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    best_preds = [round(value) for value in y_pred]\n",
        "\n",
        "    endTime =  time.perf_counter()     \n",
        "    totalRunTime = endTime - startTime   # calculate total model run time\n",
        "\n",
        "    # Evaluate predictions\n",
        "    precision = precision_score(Y_test, best_preds, average='macro')\n",
        "    accuracy = accuracy_score(Y_test, best_preds)\n",
        "    recall = recall_score(Y_test, best_preds, average='macro')\n",
        "\n",
        "    print(\"Precision = {}\".format(precision))\n",
        "    print(\"Accuracy = {}\".format(accuracy))\n",
        "    print(\"Recall = {}\".format(recall))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(Y_test, best_preds)\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Confusion Matrix')\n",
        "    fig.colorbar(cax)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot loss/error\n",
        "    results = model.evals_result()\n",
        "    epochs = len(results['validation_0']['error'])\n",
        "    x_axis = range(0, epochs)\n",
        "\n",
        "    # Log loss\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
        "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
        "    ax.legend()\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.title('XGBoost Log Loss')\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Error\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
        "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
        "    ax.legend()\n",
        "    plt.ylabel('Classification Error')\n",
        "    plt.title('XGBoost Classification Error')\n",
        "    plt.show()\n",
        "\n",
        "    # # ROC Curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(Y_test, best_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([-0.02, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    print(classification_report(Y_test, best_preds))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}